{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Code for Reproducing Table 3 of the paper:\n",
        "\n",
        "  ``**Optimal Labeler Assignment and Sampling for Active Learning in the Presence of Imperfect Labels**''\n",
        "\n",
        "---------------------------------"
      ],
      "metadata": {
        "id": "TQa6oYANrViW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Importing Packages:**"
      ],
      "metadata": {
        "id": "LmHq_IdisQBU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install git+https://github.com/modAL-python/modAL.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "NJSkzffm6FL7",
        "outputId": "c877e9e1-d2cf-4831-cc84-ba0447751beb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/modAL-python/modAL.git\n",
            "  Cloning https://github.com/modAL-python/modAL.git to /tmp/pip-req-build-_vd141g2\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/modAL-python/modAL.git /tmp/pip-req-build-_vd141g2\n",
            "  Resolved https://github.com/modAL-python/modAL.git to commit bba6f6fd00dbb862b1e09259b78caf6cffa2e755\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from modAL-python==0.4.2) (1.26.4)\n",
            "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.10/dist-packages (from modAL-python==0.4.2) (1.5.2)\n",
            "Requirement already satisfied: scipy>=0.18 in /usr/local/lib/python3.10/dist-packages (from modAL-python==0.4.2) (1.13.1)\n",
            "Requirement already satisfied: pandas>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from modAL-python==0.4.2) (2.2.2)\n",
            "Collecting skorch==0.9.0 (from modAL-python==0.4.2)\n",
            "  Downloading skorch-0.9.0-py3-none-any.whl.metadata (8.3 kB)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from skorch==0.9.0->modAL-python==0.4.2) (0.9.0)\n",
            "Requirement already satisfied: tqdm>=4.14.0 in /usr/local/lib/python3.10/dist-packages (from skorch==0.9.0->modAL-python==0.4.2) (4.66.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.0->modAL-python==0.4.2) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.0->modAL-python==0.4.2) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.0->modAL-python==0.4.2) (2024.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.18->modAL-python==0.4.2) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.18->modAL-python==0.4.2) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.1.0->modAL-python==0.4.2) (1.16.0)\n",
            "Downloading skorch-0.9.0-py3-none-any.whl (125 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.8/125.8 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: modAL-python\n",
            "  Building wheel for modAL-python (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for modAL-python: filename=modAL_python-0.4.2-py3-none-any.whl size=32647 sha256=5fd0b930ddb6207b1cde51f0057f76e0ee1b8de51279daaf1812fafbe6894cde\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-yvuh2_7z/wheels/d9/fb/59/7deb61b460c1c36394cd093758986ff7d36f71352dcb2e02c5\n",
            "Successfully built modAL-python\n",
            "Installing collected packages: skorch, modAL-python\n",
            "Successfully installed modAL-python-0.4.2 skorch-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from scipy.stats import entropy\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import f1_score,precision_score, recall_score, accuracy_score\n",
        "from modAL.models import ActiveLearner\n",
        "from modAL.uncertainty import entropy_sampling\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "emMmKCvzsIxG"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Defining Required Functions:**"
      ],
      "metadata": {
        "id": "K99mnWt9to3x"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "cEZSVEa3rSni"
      },
      "outputs": [],
      "source": [
        "def data_read(data):\n",
        "    if data == 'Heart Statlog':\n",
        "        ##########Statlog (heart) UCI\n",
        "        data = pd.read_csv('heart.dat', header =None, delimiter=' ')\n",
        "        data = np.asarray(data)\n",
        "        X = data[:,:13]\n",
        "        y = data[:,13]\n",
        "        y = y - 1\n",
        "    elif data == 'Ionosphere':\n",
        "        ###########Ionosphere\n",
        "        data = pd.read_csv('ionosphere.data' , header =None)\n",
        "        data = np.asarray(data)\n",
        "        X = data[:,0:34]\n",
        "        y = data[:,34]\n",
        "        y[[i for i in range(len(y)) if y[i] == 'g']] = 0\n",
        "        y[[i for i in range(len(y)) if y[i] == 'b']] = 1\n",
        "        y=y.astype('int')\n",
        "    elif data == 'Sonar':\n",
        "        ##########sonar data\n",
        "        data = pd.read_csv('sonar.all-data' , header =None)\n",
        "        data = np.asarray(data)\n",
        "        X = data[:,0:60]\n",
        "        y = data[:,60]\n",
        "        y[[i for i in range(len(y)) if y[i] == 'M']] = 0\n",
        "        y[[i for i in range(len(y)) if y[i] == 'R']] = 1\n",
        "        y=y.astype('int')\n",
        "    elif data == 'Spambase':\n",
        "        ###############Spmbased UCI\n",
        "        data = pd.read_csv('spambase.data',header =None)\n",
        "        data = np.asarray(data)\n",
        "        X = data[:,:57]\n",
        "        y = data[:,57]\n",
        "    else:\n",
        "        print(\" Data set not found!\")\n",
        "    return X,y\n",
        "\n",
        "def noisy_RLA(y_true, model_entropy_query):\n",
        "    Acc = copy.copy(A_boosted)\n",
        "    n = len(y_true)\n",
        "    class_list=list(range(c))\n",
        "\n",
        "    max_noise = 0\n",
        "    total_noise = 0\n",
        "    total_flip = 0\n",
        "    noise_list = []\n",
        "    y_noisy = []\n",
        "    for i in range(n):\n",
        "        random_index = np.random.randint(0, len(Acc))\n",
        "        acc_selected_labeler = Acc.pop(random_index)\n",
        "        noise = epsilon(acc_selected_labeler, model_entropy_query[i])\n",
        "        if noise > max_noise:\n",
        "            max_noise = noise\n",
        "        total_noise = total_noise + noise\n",
        "        noise_list.append(noise)\n",
        "        \"\"\"\n",
        "        higher noise, higher chance of flip\n",
        "        noise > alpha : flip\n",
        "        higher alpha returns less noisy labels_ lower alpha means harder _\n",
        "        \"\"\"\n",
        "        if alpha >= noise: #if decides correctly\n",
        "            y_noisy.append(y_true[i])\n",
        "        else: #we need to choose anyother label as a noisy one\n",
        "            new_list = [a for a in class_list if a!=y_true[i]]\n",
        "            y_noisy.append(np.random.choice(new_list))\n",
        "            total_flip = total_flip + 1\n",
        "    ######\n",
        "    average_noise = total_noise/budget\n",
        "    flip_ratio= total_flip/budget\n",
        "\n",
        "    return y_noisy, max_noise, flip_ratio, average_noise, noise_list\n",
        "######################################################################################\n",
        "def noisy_OLA(y_true, model_entropy_query):\n",
        "    Acc = copy.copy(A_boosted)\n",
        "    class_list=list(range(c))\n",
        "    n = len(y_true)\n",
        "    model_entropy_query_sorted = np.sort(model_entropy_query)\n",
        "    model_entropy_query_sorted = model_entropy_query_sorted.tolist()\n",
        "    Acc = copy.copy(A_boosted)\n",
        "    len_diff = len(Acc) - n\n",
        "    max_noise = 0\n",
        "    total_noise = 0\n",
        "    total_flip =0\n",
        "    noise_list = []\n",
        "    y_noisy=[]\n",
        "    for i in range(n):\n",
        "        idx = model_entropy_query_sorted.index( model_entropy_query[i] )\n",
        "        model_entropy_query_sorted.pop(idx)\n",
        "        acc_selected_labeler = Acc.pop((idx+len_diff))\n",
        "        noise = epsilon(acc_selected_labeler, model_entropy_query[i])\n",
        "        if noise > max_noise:\n",
        "            max_noise = noise\n",
        "        total_noise = total_noise + noise\n",
        "        noise_list.append(noise)\n",
        "        if alpha >= noise: #if decides correctly\n",
        "            y_noisy.append(y_true[i])\n",
        "        else: #we need to choose anyother label as a noisy one\n",
        "            new_list = [a for a in class_list if a!=y_true[i]]\n",
        "            y_noisy.append(np.random.choice(new_list))\n",
        "            total_flip = total_flip + 1\n",
        "    ######\n",
        "    average_noise = total_noise/budget\n",
        "    flip_ratio = total_flip/budget\n",
        "    return y_noisy, max_noise, flip_ratio, average_noise, noise_list\n",
        "######################################################################################\n",
        "def OLAS(U, U_entropy, y, beta):\n",
        "    df = pd.DataFrame(list(zip(U, U_entropy)), columns =['index', 'entropy'])\n",
        "    df = df.sort_values(by=['entropy'], ascending=False)\n",
        "    last_query = -1\n",
        "    class_list=list(range(c))\n",
        "    A_list = sorted(A, reverse = True)\n",
        "    query_indices = []\n",
        "    y_noisy = []\n",
        "    total_noise = 0\n",
        "    total_flip = 0\n",
        "    m = -1\n",
        "    flag = 1\n",
        "    while m < M-1 and flag == 1:\n",
        "        m = m + 1\n",
        "        acc = A_list[m]\n",
        "        for i in np.arange(last_query + 1, len(U)):\n",
        "            if epsilon(acc, df.iloc[i,1]) <= beta:\n",
        "                last_query = min(i + Cap -1, len(U)-1)\n",
        "                if last_query == len(U)-1 :\n",
        "                    flag = 0\n",
        "                for idx in np.arange(i, min(i + Cap, len(U))):\n",
        "                    query_indices.append(df.iloc[idx,0])\n",
        "                    noise = epsilon(acc, df.iloc[idx,1])\n",
        "                    total_noise = total_noise + noise\n",
        "                    if alpha >= noise: #if decides correctly\n",
        "                        y_noisy.append(y[df.iloc[idx,0]])\n",
        "                    else: #we need to choose anyother label as a noisy one\n",
        "                        new_list = [a for a in class_list if a!=y[df.iloc[idx,0]]]\n",
        "                        y_noisy.append(np.random.choice(new_list))\n",
        "                        total_flip = total_flip + 1\n",
        "                break # goes to main loop\n",
        "    average_noise = total_noise/len(query_indices)\n",
        "    flip_ratio = total_flip/len(query_indices)\n",
        "    return query_indices, y_noisy, flip_ratio, average_noise\n",
        "######################################################################################\n",
        "def repeat():\n",
        "    ## initialization: start with 3 pos samples and 3 negative samples\n",
        "    I = list(np.arange(n_train))\n",
        "    I_pos = [i for i in I if y_train[i] == 1]\n",
        "    I_neg = [i for i in I if y_train[i] == 0]\n",
        "    L_init_pos = random.sample(I_pos, init_number)\n",
        "    L_init_neg = random.sample(I_neg, init_number)\n",
        "    L_init = [*L_init_pos, *L_init_neg]\n",
        "    U_init = [each for each in I if each not in L_init]\n",
        "\n",
        "    ANR_RS_RLA = []\n",
        "    ANR_RS_OLA = []\n",
        "    ANR_ES_RLA = []\n",
        "    ANR_ES_OLA = []\n",
        "    ANR_OA = []\n",
        "\n",
        "    ########################################################################  RS + RLA #######################\n",
        "    learner = ActiveLearner(estimator=RandomForestClassifier(random_state=0),\n",
        "                            query_strategy=entropy_sampling,\n",
        "                            X_training=X_train[L_init],\n",
        "                            y_training=y_train[L_init])\n",
        "    L = L_init\n",
        "    U = U_init\n",
        "    for i in range(n_cycles):\n",
        "        query_indices = np.random.choice(range(len(U)), size = int(budget))\n",
        "        model_entropy_query= entropy(learner.predict_proba(X_train[U][query_indices]).T)\n",
        "        y_true = y_train[U][query_indices]\n",
        "        y_noisy, max_noise_RLA, flip_ratio_RLA, average_noise_RLA, noise_list_RLA = noisy_RLA(y_true, model_entropy_query)\n",
        "        ANR_RS_RLA.append(flip_ratio_RLA)\n",
        "        #######teaching\n",
        "        X_AL_split = X_train[U][query_indices]\n",
        "        Y_AL_split = y_noisy\n",
        "        learner.teach(X=X_AL_split, y=Y_AL_split)\n",
        "        #######Finding remaining pool of data\n",
        "        L = [*L, *np.array(U)[query_indices]]\n",
        "        U = [each for each in I if each not in L]\n",
        "\n",
        "    ## evaluation ##\n",
        "    pred_test = learner.predict(X_test)\n",
        "    F1_RS_RLA_final = f1_score(y_test, pred_test, average=avg_method)\n",
        "    prec_RS_RLA_final = precision_score(y_test, pred_test, average=avg_method)\n",
        "    recall_RS_RLA_final = recall_score(y_test, pred_test, average=avg_method)\n",
        "    acc_RS_RLA_final = accuracy_score(y_test, pred_test)\n",
        "    ANR_RS_RLA_final = np.mean(ANR_RS_RLA)\n",
        "\n",
        "    ########################################################################  RS + OLA #######################\n",
        "    learner = ActiveLearner(estimator=RandomForestClassifier(random_state=0),\n",
        "                            query_strategy=entropy_sampling,\n",
        "                            X_training=X_train[L_init],\n",
        "                            y_training=y_train[L_init])\n",
        "    L = L_init\n",
        "    U = U_init\n",
        "\n",
        "    for i in range(n_cycles):\n",
        "        query_indices = np.random.choice(range(len(U)), size = int(budget))\n",
        "        model_entropy_query= entropy(learner.predict_proba(X_train[U][query_indices]).T)\n",
        "        y_true = y_train[U][query_indices]\n",
        "        y_noisy, max_noise_OLA, flip_ratio_OLA, average_noise_OLA, noise_list_OLA = noisy_OLA(y_true, model_entropy_query)\n",
        "        ANR_RS_OLA.append(flip_ratio_OLA)\n",
        "        #######teaching\n",
        "        X_AL_split = X_train[U][query_indices]\n",
        "        Y_AL_split = y_noisy\n",
        "        learner.teach(X=X_AL_split, y=Y_AL_split)\n",
        "        #######Finding remaining pool of data\n",
        "        L = [*L, *np.array(U)[query_indices]]\n",
        "        U = [each for each in I if each not in L]\n",
        "\n",
        "    ## evaluation ##\n",
        "    pred_test = learner.predict(X_test)\n",
        "    F1_RS_OLA_final = f1_score(y_test, pred_test, average=avg_method)\n",
        "    prec_RS_OLA_final = precision_score(y_test, pred_test, average=avg_method)\n",
        "    recall_RS_OLA_final = recall_score(y_test, pred_test, average=avg_method)\n",
        "    acc_RS_OLA_final = accuracy_score(y_test, pred_test)\n",
        "    ANR_RS_OLA_final = np.mean(ANR_RS_OLA)\n",
        "\n",
        "    ########################################################################  ES + RLA #######################\n",
        "    learner = ActiveLearner(estimator=RandomForestClassifier(random_state=0),\n",
        "                            query_strategy=entropy_sampling,\n",
        "                            X_training=X_train[L_init],\n",
        "                            y_training=y_train[L_init])\n",
        "    L = L_init\n",
        "    U = U_init\n",
        "\n",
        "    for i in range(n_cycles):\n",
        "        query_indices,query_samples = learner.query(X_train[U], n_instances = int(budget))\n",
        "        model_entropy_query= entropy(learner.predict_proba(X_train[U][query_indices]).T)\n",
        "        y_true = y_train[U][query_indices]\n",
        "        y_noisy, max_noise_RLA, flip_ratio_RLA, average_noise_RLA, noise_list_RLA = noisy_RLA(y_true, model_entropy_query)\n",
        "        ANR_ES_RLA.append(flip_ratio_RLA)\n",
        "        #######teaching\n",
        "        X_AL_split = X_train[U][query_indices]\n",
        "        Y_AL_split = y_noisy\n",
        "        learner.teach(X=X_AL_split, y=Y_AL_split)\n",
        "        #######Finding remaining pool of data\n",
        "        L = [*L, *np.array(U)[query_indices]]\n",
        "        U = [each for each in I if each not in L]\n",
        "\n",
        "    ## evaluation ##\n",
        "    pred_test = learner.predict(X_test)\n",
        "    F1_ES_RLA_final = f1_score(y_test, pred_test, average=avg_method)\n",
        "    prec_ES_RLA_final = precision_score(y_test, pred_test, average=avg_method)\n",
        "    recall_ES_RLA_final = recall_score(y_test, pred_test, average=avg_method)\n",
        "    acc_ES_RLA_final = accuracy_score(y_test, pred_test)\n",
        "    ANR_ES_RLA_final = np.mean(ANR_ES_RLA)\n",
        "\n",
        "    ########################################################################  ES + OLA #######################\n",
        "    learner = ActiveLearner(estimator=RandomForestClassifier(random_state=0),\n",
        "                            query_strategy=entropy_sampling,\n",
        "                            X_training=X_train[L_init],\n",
        "                            y_training=y_train[L_init])\n",
        "    L = L_init\n",
        "    U = U_init\n",
        "\n",
        "    for i in range(n_cycles):\n",
        "        query_indices,query_samples = learner.query(X_train[U], n_instances = int(budget))\n",
        "        model_entropy_query= entropy(learner.predict_proba(X_train[query_indices]).T)\n",
        "        y_true = y_train[U][query_indices]\n",
        "        y_noisy, max_noise_OLA, flip_ratio_OLA, average_noise_OLA, noise_list_OLA = noisy_OLA(y_true, model_entropy_query)\n",
        "        ANR_ES_OLA.append(flip_ratio_OLA)\n",
        "        #######teaching\n",
        "        X_AL_split = X_train[U][query_indices]\n",
        "        Y_AL_split = y_noisy\n",
        "        learner.teach(X=X_AL_split, y=Y_AL_split)\n",
        "        #######Finding remaining pool of data\n",
        "        L = [*L, *np.array(U)[query_indices]]\n",
        "        U = [each for each in I if each not in L]\n",
        "    ## evaluation ##\n",
        "    pred_test = learner.predict(X_test)\n",
        "    F1_ES_OLA_final = f1_score(y_test, pred_test, average=avg_method)\n",
        "    prec_ES_OLA_final = precision_score(y_test, pred_test, average=avg_method)\n",
        "    recall_ES_OLA_final = recall_score(y_test, pred_test, average=avg_method)\n",
        "    acc_ES_OLA_final = accuracy_score(y_test, pred_test)\n",
        "    ANR_ES_OLA_final = np.mean(ANR_ES_OLA)\n",
        "\n",
        "    ########################################################################  OLAS #######################\n",
        "    learner = ActiveLearner(estimator=RandomForestClassifier(random_state=0),\n",
        "                            query_strategy=entropy_sampling,\n",
        "                            X_training=X_train[L_init],\n",
        "                            y_training=y_train[L_init])\n",
        "    L = L_init\n",
        "    U = U_init\n",
        "    for i in range(n_cycles):\n",
        "        U_entropy= entropy(learner.predict_proba(X_train[U]).T)\n",
        "        query_indices, y_noisy, flip_ratio, average_noise = OLAS(U, U_entropy,y_train, beta)\n",
        "        ANR_OA.append(flip_ratio)\n",
        "        #######teaching\n",
        "        X_AL_split = X_train[query_indices]\n",
        "        Y_AL_split = y_noisy\n",
        "        learner.teach(X=X_AL_split, y=Y_AL_split)\n",
        "        #######Finding remaining pool of data\n",
        "        L = [*L, *query_indices]\n",
        "        U = [each for each in I if each not in L]\n",
        "    ## evaluation ##\n",
        "    pred_test = learner.predict(X_test)\n",
        "    F1_OA_final = f1_score(y_test, pred_test, average=avg_method)\n",
        "    prec_OA_final = precision_score(y_test, pred_test, average=avg_method)\n",
        "    recall_OA_final = recall_score(y_test, pred_test, average=avg_method)\n",
        "    acc_OA_final = accuracy_score(y_test, pred_test)\n",
        "    ANR_OA_final = np.mean(ANR_OA)\n",
        "\n",
        "    return  F1_RS_RLA_final, F1_RS_OLA_final, F1_ES_RLA_final, F1_ES_OLA_final, F1_OA_final, prec_RS_RLA_final, prec_RS_OLA_final, prec_ES_RLA_final, prec_ES_OLA_final, prec_OA_final, recall_RS_RLA_final, recall_RS_OLA_final, recall_ES_RLA_final, recall_ES_OLA_final, recall_OA_final, acc_RS_RLA_final, acc_RS_OLA_final, acc_ES_RLA_final, acc_ES_OLA_final, acc_OA_final, ANR_RS_RLA_final, ANR_RS_OLA_final, ANR_ES_RLA_final, ANR_ES_OLA_final, ANR_OA_final\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Running AL:**"
      ],
      "metadata": {
        "id": "N3k4PESCxrgd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Data sets can be downloaded from: https://archive.ics.uci.edu/\n",
        "Data_Sets = ['Spambase', 'Heart Statlog' ,'Ionosphere','Sonar']\n",
        "Noise_Models = ['NM1']\n",
        "\n",
        "## parameters\n",
        "replication_num = 100\n",
        "init_number = 3 # pick 3 from each class to initialize classifier\n",
        "n_cycles = 10\n",
        "alpha = 0.2\n",
        "\n",
        "\n",
        "for data in Data_Sets:\n",
        "    ### reading data\n",
        "    X, y = data_read(data)\n",
        "    ### data split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle = True, random_state=42)\n",
        "    c = len(set(y_train))\n",
        "    if c == 2:\n",
        "        avg_method = 'binary'\n",
        "    else:\n",
        "        avg_method = 'macro'\n",
        "    n_train = X_train.shape[0]\n",
        "    ################ define dynamic parameters\n",
        "    budget = np.ceil(0.7*(n_train - 6) / 10)\n",
        "    Cap = int(np.floor(np.sqrt(budget)))\n",
        "    M = int(np.ceil(budget / Cap))\n",
        "    ### accuracy list\n",
        "    np.random.seed()\n",
        "    A = np.random.uniform(low=0.5, high=0.95, size=M)\n",
        "    A = np.sort(A)\n",
        "    A_boosted = np.array([[a]*Cap for a in A])\n",
        "    A_boosted = np.reshape(A_boosted, (M*Cap))\n",
        "    A_boosted = A_boosted.tolist()\n",
        "    beta = 0.2\n",
        "    ############################################\n",
        "    for nm in Noise_Models:\n",
        "        ### defining noise functions\n",
        "        if nm == 'NM1':\n",
        "            def epsilon(a, e): #noise model 1\n",
        "                if e <= 0:\n",
        "                    e = 0\n",
        "                elif e >= 1:\n",
        "                    e = 1\n",
        "                return e*(1-a)\n",
        "        elif nm == \"NM2\":\n",
        "            def epsilon(a, e): #noise model 2\n",
        "                if e <= 0:\n",
        "                    e = 0\n",
        "                elif e >= 1:\n",
        "                    e = 1\n",
        "                h = 0.4*e + 0.3\n",
        "                if e >=0 and e <= 0.5:\n",
        "                    p = 2*h\n",
        "                    y = (1-a**p)**(1/p)\n",
        "                else:\n",
        "                    p = 2*(1-h)\n",
        "                    y = 1-(1-(1-a)**p)**(1/p)\n",
        "                return y\n",
        "\n",
        "        # Initialize a list to collect results for each replication\n",
        "        results_list = []\n",
        "\n",
        "        for r in range(replication_num):\n",
        "            res = repeat()\n",
        "            # Append each row as a dictionary to the results list\n",
        "            results_list.append({\n",
        "                'replication': r + 1,\n",
        "                'F1 RS_RLA': res[0], 'F1 RS_OLA': res[1], 'F1 ES_RLA': res[2], 'F1 ES_OLA': res[3], 'F1 OA': res[4],\n",
        "                'precision RS_RLA': res[5], 'precision RS_OLA': res[6], 'precision ES_RLA': res[7], 'precision ES_OLA': res[8], 'precision OA': res[9],\n",
        "                'recall RS_RLA': res[10], 'recall RS_OLA': res[11], 'recall ES_RLA': res[12], 'recall ES_OLA': res[13], 'recall OA': res[14],\n",
        "                'accuracy RS_RLA': res[15], 'accuracy RS_OLA': res[16], 'accuracy ES_RLA': res[17], 'accuracy ES_OLA': res[18], 'accuracy OA': res[19],\n",
        "                'anr RS_RLA': res[20], 'anr RS_OLA': res[21], 'anr ES_RLA': res[22], 'anr ES_OLA': res[23], 'anr OA': res[24]\n",
        "            })\n",
        "\n",
        "        # Convert results_list to a DataFrame\n",
        "        results = pd.DataFrame(results_list)\n",
        "        results.to_excel('{}_{}.xlsx'.format(data, nm), index=False)\n",
        "\n",
        "        # Create the settings DataFrame without using append\n",
        "        settings = pd.DataFrame([{\n",
        "            'alpha': alpha, 'beta': beta, 'budget': budget, 'M': M, 'capacity': Cap\n",
        "        }])\n",
        "        settings.to_excel('{}_{}_Settings.xlsx'.format(data, nm), index=False)\n",
        "\n",
        "\n",
        "        print(\"Data set {} with noise model {} is done!\".format(data,nm))"
      ],
      "metadata": {
        "id": "Mei8DQpoxu6F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Outputting Results:**"
      ],
      "metadata": {
        "id": "OGg3czEc0E4_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Data_Sets = ['Heart Statlog' ,'Ionosphere','Sonar','Spambase'] #'Heart Statlog' ,'Ionosphere','Sonar','Spambase'\n",
        "Noise_Models = ['NM1']\n",
        "\n",
        "for nm in Noise_Models:\n",
        "    print(\"________________ noise model:\",nm)\n",
        "    for data in Data_Sets:\n",
        "        print(\"*********data set: \",data)\n",
        "        df = pd.read_excel('{}_{}.xlsx'.format(data, nm))\n",
        "        print(\"RS_RLA:   ${} \\pm {}$ \".format(round(df['F1 RS_RLA'].mean(),3), round(df['F1 RS_RLA'].std(),3)))\n",
        "        print(\"RS_OLA:   ${} \\pm {}$ \".format(round(df['F1 RS_OLA'].mean(),3), round(df['F1 RS_OLA'].std(),3)))\n",
        "        print(\"ES_RLA:   ${} \\pm {}$ \".format(round(df['F1 ES_RLA'].mean(),3), round(df['F1 ES_RLA'].std(),3)))\n",
        "        print(\"ES_OLA:   ${} \\pm {}$ \".format(round(df['F1 ES_OLA'].mean(),3), round(df['F1 ES_OLA'].std(),3)))\n",
        "        print(\"OLAS:   ${} \\pm {}$ \".format(round(df['F1 OA'].mean(),3), round(df['F1 OA'].std(),3)))\n"
      ],
      "metadata": {
        "id": "APHhNRkG0Hqv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}